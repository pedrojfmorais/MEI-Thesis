{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMJgoGBoxZbl",
        "outputId": "d2cf12d4-4b6b-4535-be08-c7c32317e933"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qwbXR5M7jJp",
        "outputId": "6413a89a-2113-4747-a1ef-e181fac377de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏬ Downloading https://github.com/jaimergp/miniforge/releases/download/24.11.2-1_colab/Miniforge3-colab-24.11.2-1_colab-Linux-x86_64.sh...\n",
            "📦 Installing...\n",
            "📌 Adjusting configuration...\n",
            "🩹 Patching environment...\n",
            "⏲ Done in 0:00:10\n",
            "🔁 Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RLH6mm-T7oDg",
        "outputId": "2178b8e8-3484-4dbc-cdf9-ceeca104fe3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EnvironmentLocationNotFound: Not a conda environment: /usr/local/envs/myenv\n",
            "\n",
            "Channels:\n",
            " - conda-forge\n",
            "Platform: linux-64\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "    current version: 24.11.2\n",
            "    latest version: 25.5.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/myenv\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.10\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ca-certificates-2025.6.15  |       hbd8a1cb_0         148 KB  conda-forge\n",
            "    ld_impl_linux-64-2.43      |       h1423503_5         655 KB  conda-forge\n",
            "    libexpat-2.7.0             |       h5888daf_0          73 KB  conda-forge\n",
            "    libffi-3.4.6               |       h2dba641_1          56 KB  conda-forge\n",
            "    libgcc-15.1.0              |       h767d61c_2         810 KB  conda-forge\n",
            "    libgcc-ng-15.1.0           |       h69a702a_2          34 KB  conda-forge\n",
            "    libgomp-15.1.0             |       h767d61c_2         442 KB  conda-forge\n",
            "    liblzma-5.8.1              |       hb9d3cd8_2         110 KB  conda-forge\n",
            "    libnsl-2.0.1               |       hb9d3cd8_1          33 KB  conda-forge\n",
            "    libsqlite-3.50.1           |       h6cd9bfd_6         898 KB  conda-forge\n",
            "    ncurses-6.5                |       h2d0b736_3         871 KB  conda-forge\n",
            "    openssl-3.5.0              |       h7b32b05_1         3.0 MB  conda-forge\n",
            "    pip-25.1.1                 |     pyh8b19718_0         1.2 MB  conda-forge\n",
            "    python-3.10.18             |hd6af730_0_cpython        23.9 MB  conda-forge\n",
            "    readline-8.2               |       h8c095d6_2         276 KB  conda-forge\n",
            "    setuptools-80.9.0          |     pyhff2d567_0         731 KB  conda-forge\n",
            "    tk-8.6.13                  |noxft_hd72426e_102         3.1 MB  conda-forge\n",
            "    tzdata-2025b               |       h78e105d_0         120 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        36.3 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge \n",
            "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu \n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h4bc722e_7 \n",
            "  ca-certificates    conda-forge/noarch::ca-certificates-2025.6.15-hbd8a1cb_0 \n",
            "  ld_impl_linux-64   conda-forge/linux-64::ld_impl_linux-64-2.43-h1423503_5 \n",
            "  libexpat           conda-forge/linux-64::libexpat-2.7.0-h5888daf_0 \n",
            "  libffi             conda-forge/linux-64::libffi-3.4.6-h2dba641_1 \n",
            "  libgcc             conda-forge/linux-64::libgcc-15.1.0-h767d61c_2 \n",
            "  libgcc-ng          conda-forge/linux-64::libgcc-ng-15.1.0-h69a702a_2 \n",
            "  libgomp            conda-forge/linux-64::libgomp-15.1.0-h767d61c_2 \n",
            "  liblzma            conda-forge/linux-64::liblzma-5.8.1-hb9d3cd8_2 \n",
            "  libnsl             conda-forge/linux-64::libnsl-2.0.1-hb9d3cd8_1 \n",
            "  libsqlite          conda-forge/linux-64::libsqlite-3.50.1-h6cd9bfd_6 \n",
            "  libuuid            conda-forge/linux-64::libuuid-2.38.1-h0b41bf4_0 \n",
            "  libxcrypt          conda-forge/linux-64::libxcrypt-4.4.36-hd590300_1 \n",
            "  libzlib            conda-forge/linux-64::libzlib-1.3.1-hb9d3cd8_2 \n",
            "  ncurses            conda-forge/linux-64::ncurses-6.5-h2d0b736_3 \n",
            "  openssl            conda-forge/linux-64::openssl-3.5.0-h7b32b05_1 \n",
            "  pip                conda-forge/noarch::pip-25.1.1-pyh8b19718_0 \n",
            "  python             conda-forge/linux-64::python-3.10.18-hd6af730_0_cpython \n",
            "  readline           conda-forge/linux-64::readline-8.2-h8c095d6_2 \n",
            "  setuptools         conda-forge/noarch::setuptools-80.9.0-pyhff2d567_0 \n",
            "  tk                 conda-forge/linux-64::tk-8.6.13-noxft_hd72426e_102 \n",
            "  tzdata             conda-forge/noarch::tzdata-2025b-h78e105d_0 \n",
            "  wheel              conda-forge/noarch::wheel-0.45.1-pyhd8ed1ab_1 \n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages:\n",
            "python-3.10.18       | 23.9 MB   | :   0% 0/1 [00:00<?, ?it/s]\n",
            "tk-8.6.13            | 3.1 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "openssl-3.5.0        | 3.0 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pip-25.1.1           | 1.2 MB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.50.1     | 898 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 871 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-15.1.0        | 810 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-80.9.0    | 731 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 655 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-15.1.0       | 442 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "readline-8.2         | 276 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 148 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tzdata-2025b         | 120 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-5.8.1        | 110 KB    | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libexpat-2.7.0       | 73 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libffi-3.4.6         | 56 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-15.1.0     | 34 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libnsl-2.0.1         | 33 KB     | :   0% 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "python-3.10.18       | 23.9 MB   | :   4% 0.03729270714749733/1 [00:00<00:02,  2.70s/it]\n",
            "tk-8.6.13            | 3.1 MB    | :  89% 0.8927104679039719/1 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.50.1     | 898 KB    | :  71% 0.7127638632549651/1 [00:00<00:00,  7.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.50.1     | 898 KB    | : 100% 1.0/1 [00:00<00:00,  7.13it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 871 KB    | :   2% 0.018375108367605347/1 [00:00<00:06,  6.45s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "tk-8.6.13            | 3.1 MB    | : 100% 1.0/1 [00:00<00:00,  8.89it/s]               \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 871 KB    | : 100% 1.0/1 [00:00<00:00,  6.45s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-15.1.0        | 810 KB    | :   2% 0.019760996154903825/1 [00:00<00:07,  7.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-15.1.0        | 810 KB    | : 100% 1.0/1 [00:00<00:00,  7.22s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-80.9.0    | 731 KB    | :   2% 0.021880692532465797/1 [00:00<00:07,  7.20s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-80.9.0    | 731 KB    | : 100% 1.0/1 [00:00<00:00,  7.20s/it]                 \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 655 KB    | :   2% 0.02443057699046426/1 [00:00<00:06,  7.05s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-15.1.0       | 442 KB    | :   4% 0.03619693572083467/1 [00:00<00:04,  5.16s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-15.1.0       | 442 KB    | : 100% 1.0/1 [00:00<00:00,  5.16s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 655 KB    | : 100% 1.0/1 [00:00<00:00,  7.05s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "openssl-3.5.0        | 3.0 MB    | :   1% 0.005255644910358278/1 [00:00<00:38, 38.44s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.10.18       | 23.9 MB   | :  22% 0.22179346882458936/1 [00:00<00:00,  1.14it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "readline-8.2         | 276 KB    | : 100% 1.0/1 [00:00<00:00,  3.47s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "libsqlite-3.50.1     | 898 KB    | : 100% 1.0/1 [00:00<00:00,  7.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tzdata-2025b         | 120 KB    | :  13% 0.1332379155552664/1 [00:00<00:01,  1.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tzdata-2025b         | 120 KB    | : 100% 1.0/1 [00:00<00:00,  1.77s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-5.8.1        | 110 KB    | :  15% 0.1451272875440679/1 [00:00<00:01,  1.65s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 148 KB    | :  11% 0.10845375292085074/1 [00:00<00:02,  2.25s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-5.8.1        | 110 KB    | : 100% 1.0/1 [00:00<00:00,  1.65s/it]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 148 KB    | : 100% 1.0/1 [00:00<00:00,  2.25s/it]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "openssl-3.5.0        | 3.0 MB    | : 100% 1.0/1 [00:00<00:00, 38.44s/it]                 \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libexpat-2.7.0       | 73 KB     | :  22% 0.22013516600158545/1 [00:00<00:00,  1.25s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libffi-3.4.6         | 56 KB     | :  29% 0.2852715337871955/1 [00:00<00:00,  1.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-15.1.0     | 34 KB     | :  47% 0.47371768923842017/1 [00:00<00:00,  1.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libffi-3.4.6         | 56 KB     | : 100% 1.0/1 [00:00<00:00,  1.01it/s]               \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-15.1.0     | 34 KB     | : 100% 1.0/1 [00:00<00:00,  1.67it/s]                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.10.18       | 23.9 MB   | :  40% 0.40236868238089224/1 [00:00<00:00,  1.41it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libnsl-2.0.1         | 33 KB     | :  49% 0.4857252972043521/1 [00:00<00:00,  1.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "python-3.10.18       | 23.9 MB   | : 100% 1.0/1 [00:00<00:00,  1.73it/s]\n",
            "\n",
            "\n",
            "pip-25.1.1           | 1.2 MB    | : 100% 1.0/1 [00:00<00:00,  1.58it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "pip-25.1.1           | 1.2 MB    | : 100% 1.0/1 [00:00<00:00,  1.58it/s]\u001b[A\u001b[A\u001b[A\n",
            "tk-8.6.13            | 3.1 MB    | : 100% 1.0/1 [00:00<00:00,  8.89it/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-15.1.0        | 810 KB    | : 100% 1.0/1 [00:00<00:00,  1.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-15.1.0        | 810 KB    | : 100% 1.0/1 [00:00<00:00,  1.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-80.9.0    | 731 KB    | : 100% 1.0/1 [00:01<00:00,  1.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "setuptools-80.9.0    | 731 KB    | : 100% 1.0/1 [00:01<00:00,  1.19s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-15.1.0       | 442 KB    | : 100% 1.0/1 [00:01<00:00,  1.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgomp-15.1.0       | 442 KB    | : 100% 1.0/1 [00:01<00:00,  1.24s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 655 KB    | : 100% 1.0/1 [00:01<00:00,  1.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ld_impl_linux-64-2.4 | 655 KB    | : 100% 1.0/1 [00:01<00:00,  1.30s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "readline-8.2         | 276 KB    | : 100% 1.0/1 [00:01<00:00,  1.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "readline-8.2         | 276 KB    | : 100% 1.0/1 [00:01<00:00,  1.34s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tzdata-2025b         | 120 KB    | : 100% 1.0/1 [00:01<00:00,  1.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tzdata-2025b         | 120 KB    | : 100% 1.0/1 [00:01<00:00,  1.74s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-5.8.1        | 110 KB    | : 100% 1.0/1 [00:01<00:00,  1.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "liblzma-5.8.1        | 110 KB    | : 100% 1.0/1 [00:01<00:00,  1.77s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 148 KB    | : 100% 1.0/1 [00:01<00:00,  1.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ca-certificates-2025 | 148 KB    | : 100% 1.0/1 [00:01<00:00,  1.76s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 871 KB    | : 100% 1.0/1 [00:01<00:00,  1.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ncurses-6.5          | 871 KB    | : 100% 1.0/1 [00:01<00:00,  1.94s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libffi-3.4.6         | 56 KB     | : 100% 1.0/1 [00:01<00:00,  2.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libffi-3.4.6         | 56 KB     | : 100% 1.0/1 [00:01<00:00,  2.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "openssl-3.5.0        | 3.0 MB    | : 100% 1.0/1 [00:02<00:00,  1.95s/it]\u001b[A\u001b[A\n",
            "\n",
            "openssl-3.5.0        | 3.0 MB    | : 100% 1.0/1 [00:02<00:00,  1.95s/it]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-15.1.0     | 34 KB     | : 100% 1.0/1 [00:02<00:00,  2.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libgcc-ng-15.1.0     | 34 KB     | : 100% 1.0/1 [00:02<00:00,  2.23s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libexpat-2.7.0       | 73 KB     | : 100% 1.0/1 [00:02<00:00,  2.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libexpat-2.7.0       | 73 KB     | : 100% 1.0/1 [00:02<00:00,  2.07s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "libnsl-2.0.1         | 33 KB     | : 100% 1.0/1 [00:02<00:00,  2.26s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \n",
            "                                                                        \u001b[A\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "                                                                        \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "Preparing transaction: - \b\b\\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate myenv\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n",
            "Collecting absl-py==2.1.0 (from -r requirements.txt (line 1))\n",
            "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting astor==0.8.1 (from -r requirements.txt (line 2))\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting astunparse==1.6.3 (from -r requirements.txt (line 3))\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting beautifulsoup4==4.13.3 (from -r requirements.txt (line 4))\n",
            "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting certifi==2025.1.31 (from -r requirements.txt (line 5))\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting charset-normalizer==3.4.1 (from -r requirements.txt (line 6))\n",
            "  Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting cloudpickle==3.1.1 (from -r requirements.txt (line 7))\n",
            "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting docopt==0.6.2 (from -r requirements.txt (line 8))\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gast==0.4.0 (from -r requirements.txt (line 9))\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting google-pasta==0.2.0 (from -r requirements.txt (line 10))\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting grpcio==1.58.0 (from -r requirements.txt (line 11))\n",
            "  Downloading grpcio-1.58.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting h5py==3.8.0 (from -r requirements.txt (line 12))\n",
            "  Downloading h5py-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting idna==3.10 (from -r requirements.txt (line 13))\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting importlib_metadata==6.8.0 (from -r requirements.txt (line 14))\n",
            "  Downloading importlib_metadata-6.8.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting joblib==1.3.2 (from -r requirements.txt (line 15))\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting Js2Py==0.74 (from -r requirements.txt (line 16))\n",
            "  Downloading Js2Py-0.74-py3-none-any.whl.metadata (868 bytes)\n",
            "Collecting keras==2.10.0 (from -r requirements.txt (line 17))\n",
            "  Downloading keras-2.10.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting Keras-Applications==1.0.8 (from -r requirements.txt (line 18))\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting Keras-Preprocessing==1.1.2 (from -r requirements.txt (line 19))\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting Markdown==3.5.2 (from -r requirements.txt (line 20))\n",
            "  Downloading Markdown-3.5.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting MarkupSafe==2.1.5 (from -r requirements.txt (line 21))\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting mtcnn==0.1.1 (from -r requirements.txt (line 22))\n",
            "  Downloading mtcnn-0.1.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting numpy==1.23.5 (from -r requirements.txt (line 23))\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting opencv-python==4.7.0.72 (from -r requirements.txt (line 24))\n",
            "  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting opt_einsum==3.3.0 (from -r requirements.txt (line 25))\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting packaging==24.2 (from -r requirements.txt (line 26))\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pandas==1.5.3 (from -r requirements.txt (line 27))\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting pillow==10.4.0 (from -r requirements.txt (line 28))\n",
            "  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting psutil==5.9.8 (from -r requirements.txt (line 29))\n",
            "  Downloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting pyjsparser==2.7.1 (from -r requirements.txt (line 30))\n",
            "  Downloading pyjsparser-2.7.1.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting PyPrind==2.11.3 (from -r requirements.txt (line 31))\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pySmartDL==1.3.4 (from -r requirements.txt (line 32))\n",
            "  Downloading pySmartDL-1.3.4-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting python-dateutil==2.9.0.post0 (from -r requirements.txt (line 33))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz==2023.3 (from -r requirements.txt (line 34))\n",
            "  Downloading pytz-2023.3-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting PyYAML==6.0.2 (from -r requirements.txt (line 35))\n",
            "  Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting requests==2.31.0 (from -r requirements.txt (line 36))\n",
            "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting scikit-learn==1.2.2 (from -r requirements.txt (line 37))\n",
            "  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting scipy==1.10.1 (from -r requirements.txt (line 38))\n",
            "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Collecting six==1.17.0 (from -r requirements.txt (line 39))\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting soupsieve==2.6 (from -r requirements.txt (line 40))\n",
            "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tensorboard==2.10.1 (from -r requirements.txt (line 41))\n",
            "  Downloading tensorboard-2.10.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting tensorflow-estimator==2.10.0 (from -r requirements.txt (line 42))\n",
            "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting tensorflow==2.10.0 (from -r requirements.txt (line 43))\n",
            "  Downloading tensorflow-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n",
            "Collecting termcolor==2.4.0 (from -r requirements.txt (line 44))\n",
            "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting threadpoolctl==3.5.0 (from -r requirements.txt (line 45))\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting torch==1.13.1 (from -r requirements.txt (line 46))\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting torchaudio==0.13.1 (from -r requirements.txt (line 47))\n",
            "  Downloading torchaudio-0.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting torchvision==0.14.1 (from -r requirements.txt (line 48))\n",
            "  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "Collecting tqdm==4.66.1 (from -r requirements.txt (line 49))\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting typing_extensions==4.12.2 (from -r requirements.txt (line 50))\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting tzlocal==5.2 (from -r requirements.txt (line 51))\n",
            "  Downloading tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting urllib3==2.2.3 (from -r requirements.txt (line 52))\n",
            "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting webencodings==0.5.1 (from -r requirements.txt (line 53))\n",
            "  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting Werkzeug==3.0.1 (from -r requirements.txt (line 54))\n",
            "  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting wrapt==1.17.2 (from -r requirements.txt (line 55))\n",
            "  Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting zipp==3.15.0 (from -r requirements.txt (line 56))\n",
            "  Downloading zipp-3.15.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/envs/myenv/lib/python3.10/site-packages (from astunparse==1.6.3->-r requirements.txt (line 3)) (0.45.1)\n",
            "Collecting google-auth<3,>=1.6.3 (from tensorboard==2.10.1->-r requirements.txt (line 41))\n",
            "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard==2.10.1->-r requirements.txt (line 41))\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorboard==2.10.1->-r requirements.txt (line 41))\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/envs/myenv/lib/python3.10/site-packages (from tensorboard==2.10.1->-r requirements.txt (line 41)) (80.9.0)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard==2.10.1->-r requirements.txt (line 41))\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard==2.10.1->-r requirements.txt (line 41))\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
            "Collecting flatbuffers>=2.0 (from tensorflow==2.10.0->-r requirements.txt (line 43))\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow==2.10.0->-r requirements.txt (line 43))\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow==2.10.0->-r requirements.txt (line 43))\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1->-r requirements.txt (line 46))\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1->-r requirements.txt (line 46))\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1->-r requirements.txt (line 46))\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1->-r requirements.txt (line 46))\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard==2.10.1->-r requirements.txt (line 41))\n",
            "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard==2.10.1->-r requirements.txt (line 41))\n",
            "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard==2.10.1->-r requirements.txt (line 41))\n",
            "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.10.1->-r requirements.txt (line 41))\n",
            "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard==2.10.1->-r requirements.txt (line 41))\n",
            "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.10.1->-r requirements.txt (line 41))\n",
            "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
            "Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
            "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
            "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Downloading grpcio-1.58.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h5py-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\n",
            "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "Downloading Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Downloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m146.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m159.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m125.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psutil-5.9.8-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Downloading pySmartDL-1.3.4-py3-none-any.whl (20 kB)\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
            "Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
            "Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m143.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
            "Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m137.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
            "Downloading tensorflow-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.0/578.0 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-0.13.1-cp310-cp310-manylinux1_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m141.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m142.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
            "Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
            "Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
            "Downloading wrapt-1.17.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
            "Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m190.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
            "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
            "Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
            "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m140.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m180.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
            "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
            "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
            "Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m133.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docopt, pyjsparser\n",
            "\u001b[33m  DEPRECATION: Building 'docopt' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'docopt'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13783 sha256=14ef12c4a596b971d7604fb291b75409774654450a0017f61c9c3aad42f9c9fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "\u001b[33m  DEPRECATION: Building 'pyjsparser' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pyjsparser'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for pyjsparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyjsparser: filename=pyjsparser-2.7.1-py3-none-any.whl size=26010 sha256=fd3d364322f9802b846d59b491264fe09825ba796719d121b7ae1bb6499b0c66\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/81/26/5956478df303e2bf5a85a5df595bb307bd25948a4bab69f7c7\n",
            "Successfully built docopt pyjsparser\n",
            "Installing collected packages: webencodings, tensorboard-plugin-wit, pytz, pySmartDL, PyPrind, pyjsparser, libclang, keras, flatbuffers, docopt, zipp, wrapt, urllib3, tzlocal, typing_extensions, tqdm, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, soupsieve, six, PyYAML, pyasn1, psutil, protobuf, pillow, packaging, oauthlib, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, numpy, MarkupSafe, Markdown, joblib, idna, grpcio, gast, cloudpickle, charset-normalizer, certifi, cachetools, astor, absl-py, Werkzeug, scipy, rsa, requests, python-dateutil, pyasn1-modules, opt_einsum, opencv-python, nvidia-cudnn-cu11, Keras-Preprocessing, Js2Py, importlib_metadata, h5py, google-pasta, beautifulsoup4, astunparse, torch, scikit-learn, requests-oauthlib, pandas, mtcnn, Keras-Applications, google-auth, torchvision, torchaudio, google-auth-oauthlib, tensorboard, tensorflow\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74/74\u001b[0m [tensorflow]\n",
            "\u001b[1A\u001b[2KSuccessfully installed Js2Py-0.74 Keras-Applications-1.0.8 Keras-Preprocessing-1.1.2 Markdown-3.5.2 MarkupSafe-2.1.5 PyPrind-2.11.3 PyYAML-6.0.2 Werkzeug-3.0.1 absl-py-2.1.0 astor-0.8.1 astunparse-1.6.3 beautifulsoup4-4.13.3 cachetools-5.5.2 certifi-2025.1.31 charset-normalizer-3.4.1 cloudpickle-3.1.1 docopt-0.6.2 flatbuffers-25.2.10 gast-0.4.0 google-auth-2.40.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.58.0 h5py-3.8.0 idna-3.10 importlib_metadata-6.8.0 joblib-1.3.2 keras-2.10.0 libclang-18.1.1 mtcnn-0.1.1 numpy-1.23.5 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 oauthlib-3.3.1 opencv-python-4.7.0.72 opt_einsum-3.3.0 packaging-24.2 pandas-1.5.3 pillow-10.4.0 protobuf-3.19.6 psutil-5.9.8 pySmartDL-1.3.4 pyasn1-0.6.1 pyasn1-modules-0.4.2 pyjsparser-2.7.1 python-dateutil-2.9.0.post0 pytz-2023.3 requests-2.31.0 requests-oauthlib-2.0.0 rsa-4.9.1 scikit-learn-1.2.2 scipy-1.10.1 six-1.17.0 soupsieve-2.6 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.4.0 threadpoolctl-3.5.0 torch-1.13.1 torchaudio-0.13.1 torchvision-0.14.1 tqdm-4.66.1 typing_extensions-4.12.2 tzlocal-5.2 urllib3-2.2.3 webencodings-0.5.1 wrapt-1.17.2 zipp-3.15.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Create venv with Python 3.10 and Install requirements\n",
        "%%shell\n",
        "conda remove --name myenv --all -y\n",
        "conda create --name myenv python=3.10\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate myenv\n",
        "cd /content/drive/MyDrive/mad/code/ViT-SVM/\n",
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvkLGsBSkvKl",
        "outputId": "ed087dbb-de5f-4fb9-800f-0784a5a7d748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,750 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,572 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,726 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,059 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,036 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,347 kB]\n",
            "Fetched 27.9 MB in 4s (7,243 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "jq is already the newest version (1.6-2.1ubuntu3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "#Install dependencies\n",
        "\n",
        "apt-get update && apt-get install -y jq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvNGiIW0efvf"
      },
      "outputs": [],
      "source": [
        "class DatasetConfig:\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_name: str,\n",
        "        is_cropped_images: bool,\n",
        "        morph_images_path: str,\n",
        "        bonafide_images_path: str,\n",
        "        images_suffix: str,\n",
        "        features_path: str,\n",
        "        model_path: str,\n",
        "        results_path: str\n",
        "    ):\n",
        "        self.dataset_name = dataset_name\n",
        "        self.is_cropped_images = is_cropped_images\n",
        "        self.morph_images_path = morph_images_path\n",
        "        self.bonafide_images_path = bonafide_images_path\n",
        "        self.images_suffix = images_suffix\n",
        "        self.features_path = features_path\n",
        "        self.model_path = model_path\n",
        "        self.results_path = results_path\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (\n",
        "            f\"DatasetConfig(dataset_name='{self.dataset_name}', \"\n",
        "            f\"is_cropped_images={self.is_cropped_images}, \"\n",
        "            f\"morph_images_path='{self.morph_images_path}', \"\n",
        "            f\"bonafide_images_path='{self.bonafide_images_path}', \"\n",
        "            f\"images_suffix='{self.images_suffix}', \"\n",
        "            f\"features_path='{self.features_path}', \"\n",
        "            f\"model_path='{self.model_path}', \"\n",
        "            f\"results_path='{self.results_path}')\"\n",
        "        )\n",
        "\n",
        "dataset_configs = {\n",
        "    \"ffhq\": DatasetConfig(\n",
        "        dataset_name=\"FFHQ - Not cropped\",\n",
        "        is_cropped_images=False,\n",
        "        morph_images_path=\"/content/drive/MyDrive/mad/datasets/ffhq-morphs/\",\n",
        "        bonafide_images_path=\"/content/drive/MyDrive/mad/datasets/ffhq-dataset/images1024x1024/\",\n",
        "        images_suffix=\"png\",\n",
        "        features_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/extracted_features/ffhq-uncroppped/\",\n",
        "        model_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/trained_classifiers/ffhq-uncroppped/\",\n",
        "        results_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/ffhq-uncroppped/\"\n",
        "    ),\n",
        "    \"ffhq-cropped\": DatasetConfig(\n",
        "        dataset_name=\"FFHQ - Cropped\",\n",
        "        is_cropped_images=True,\n",
        "        morph_images_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/datasets/ffhq-cropped/morphs/\",\n",
        "        bonafide_images_path=\"/content/drive/MyDrive/mad/datasets/ffhq-dataset/images1024x1024/\",\n",
        "        images_suffix=\"png\",\n",
        "        features_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/extracted_features/ffhq-cropped/\",\n",
        "        model_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/trained_classifiers/ffhq-cropped/\",\n",
        "        results_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/ffhq-cropped/\"\n",
        "    ),\n",
        "\n",
        "    \"frll\": DatasetConfig(\n",
        "        dataset_name=\"FRLL - Not cropped\",\n",
        "        is_cropped_images=False,\n",
        "        morph_images_path=\"/content/drive/MyDrive/mad/datasets/frll-morphs/\",\n",
        "        bonafide_images_path=\"/content/drive/MyDrive/mad/datasets/frll-morphs/raw/\",\n",
        "        images_suffix=\"jpg\",\n",
        "        features_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/extracted_features/frll-uncroppped/\",\n",
        "        model_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/trained_classifiers/frll-uncroppped/\",\n",
        "        results_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/frll-uncroppped/\"\n",
        "    ),\n",
        "    \"frll-cropped\": DatasetConfig(\n",
        "        dataset_name=\"FRLL - Cropped\",\n",
        "        is_cropped_images=True,\n",
        "        morph_images_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/datasets/frll-cropped/morphs/\",\n",
        "        bonafide_images_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/datasets/frll-cropped/bonafide/\",\n",
        "        images_suffix=\"jpg\",\n",
        "        features_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/extracted_features/frll-cropped/\",\n",
        "        model_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/trained_classifiers/frll-cropped/\",\n",
        "        results_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/frll-cropped/\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "def map_to_cropped_versions(configs: dict) -> dict:\n",
        "    cropped_map = {}\n",
        "    for key in configs:\n",
        "        if not key.endswith(\"-cropped\"):\n",
        "            cropped_key = f\"{key}-cropped\"\n",
        "            if cropped_key in configs:\n",
        "                cropped_map[key] = cropped_key\n",
        "    return cropped_map\n",
        "\n",
        "cropped_dataset_map = map_to_cropped_versions(dataset_configs)\n",
        "\n",
        "import json\n",
        "\n",
        "def serialize_dataset_configs(configs):\n",
        "    return {\n",
        "        k: {\n",
        "            \"dataset_name\": v.dataset_name,\n",
        "            \"is_cropped_images\": v.is_cropped_images,\n",
        "            \"morph_images_path\": v.morph_images_path,\n",
        "            \"bonafide_images_path\": v.bonafide_images_path,\n",
        "            \"images_suffix\": v.images_suffix,\n",
        "            \"features_path\": v.features_path,\n",
        "            \"model_path\": v.model_path,\n",
        "            \"results_path\": v.results_path,\n",
        "        }\n",
        "        for k, v in configs.items()\n",
        "    }\n",
        "\n",
        "# Save dataset configs\n",
        "with open(\"dataset_configs.json\", \"w\") as f:\n",
        "    json.dump(serialize_dataset_configs(dataset_configs), f)\n",
        "\n",
        "# Save cropped map\n",
        "with open(\"cropped_dataset_map.json\", \"w\") as f:\n",
        "    json.dump(cropped_dataset_map, f)\n",
        "\n",
        "!cp dataset_configs.json /content/drive/MyDrive/mad/code/ViT-SVM/configs/dataset_configs.json\n",
        "!cp cropped_dataset_map.json /content/drive/MyDrive/mad/code/ViT-SVM/configs/cropped_dataset_map.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lXrbLh6IjUGr",
        "outputId": "eb15157a-04a0-4bb2-99cb-64d6e5dcd159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️ Skipping cropping step (set RUN_CROP=1 to enable).\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "# Crop images\n",
        "\n",
        "# Set this flag to 1 to enable cropping, 0 to skip\n",
        "RUN_CROP=0\n",
        "\n",
        "if [ \"$RUN_CROP\" -eq 1 ]; then\n",
        "  echo \"🚀 Running cropping process...\"\n",
        "\n",
        "  eval \"$(conda shell.bash hook)\"\n",
        "  conda activate myenv\n",
        "\n",
        "  # Loop through all keys in the cropped map\n",
        "  for ORIGINAL_KEY in $(jq -r 'keys[]' cropped_dataset_map.json); do\n",
        "    CROPPED_KEY=$(jq -r --arg k \"$ORIGINAL_KEY\" '.[$k]' cropped_dataset_map.json)\n",
        "\n",
        "    for TYPE in morph bonafide; do\n",
        "      # Skip if ffhq and bonafide\n",
        "      if [[ \"$ORIGINAL_KEY\" == \"ffhq\" && \"$TYPE\" == \"bonafide\" ]]; then\n",
        "        echo \"⏩ Skipping $TYPE for $ORIGINAL_KEY\"\n",
        "        continue\n",
        "      fi\n",
        "\n",
        "      SOURCE_PATH=$(jq -r --arg k \"$ORIGINAL_KEY\" --arg t \"$TYPE\" '.[$k][$t + \"_images_path\"]' dataset_configs.json)\n",
        "      TARGET_PATH=$(jq -r --arg k \"$CROPPED_KEY\" --arg t \"$TYPE\" '.[$k][$t + \"_images_path\"]' dataset_configs.json)\n",
        "      SUFFIX=$(jq -r --arg k \"$ORIGINAL_KEY\" '.[$k].images_suffix' dataset_configs.json)\n",
        "\n",
        "      echo \"🔄 Processing $TYPE for $ORIGINAL_KEY → $CROPPED_KEY\"\n",
        "      echo \"   Source: $SOURCE_PATH\"\n",
        "      echo \"   Target: $TARGET_PATH\"\n",
        "      echo \"   Suffix: $SUFFIX\"\n",
        "\n",
        "      mkdir -p \"$TARGET_PATH\"\n",
        "\n",
        "      for SUBDIR in \"$SOURCE_PATH\"/*; do\n",
        "        if [ -d \"$SUBDIR\" ]; then\n",
        "          SUBFOLDER_NAME=$(basename \"$SUBDIR\")\n",
        "          SOURCE_SUBDIR=\"$SUBDIR\"\n",
        "          TARGET_SUBDIR=\"$TARGET_PATH/$SUBFOLDER_NAME\"\n",
        "\n",
        "          echo \"📁 Processing $SUBFOLDER_NAME...\"\n",
        "          python /content/drive/MyDrive/mad/code/ViT-SVM/src/crop_by_MTCNN.py \\\n",
        "            --source_dir \"$SOURCE_SUBDIR\" \\\n",
        "            --target_dir \"$TARGET_SUBDIR\" \\\n",
        "            --source_suffix \"$SUFFIX\"\n",
        "        fi\n",
        "      done\n",
        "    done\n",
        "  done\n",
        "else\n",
        "  echo \"⚠️ Skipping cropping step (set RUN_CROP=1 to enable).\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2GYqONPnOrp",
        "outputId": "7dc7e54b-7358-47fd-f707-b4e005cda31c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Skipping ffhq (bonafide) - output already exists.\n",
            "✅ Skipping ffhq (morph) - output already exists.\n",
            "✅ Skipping ffhq-cropped (bonafide) - output already exists.\n",
            "✅ Skipping ffhq-cropped (morph) - output already exists.\n",
            "✅ Skipping frll (bonafide) - output already exists.\n",
            "✅ Skipping frll (morph) - output already exists.\n",
            "✅ Skipping frll-cropped (bonafide) - output already exists.\n",
            "✅ Skipping frll-cropped (morph) - output already exists.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "# Extract features\n",
        "\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate myenv\n",
        "\n",
        "for DATASET_KEY in $(jq -r 'keys[]' dataset_configs.json); do\n",
        "  for TYPE in bonafide morph; do\n",
        "    DATA_DIR=$(jq -r --arg key \"$DATASET_KEY\" --arg t \"$TYPE\" '.[$key][($t + \"_images_path\")]' dataset_configs.json)\n",
        "    BASE_OUTPUT_DIR=$(jq -r --arg key \"$DATASET_KEY\" '.[$key].features_path' dataset_configs.json)\n",
        "    OUTPUT_DIR=\"${BASE_OUTPUT_DIR}/${TYPE}\"   # Append morph or bonafide here\n",
        "\n",
        "    if [[ -f \"$OUTPUT_DIR/filenames.csv\" && -f \"$OUTPUT_DIR/features.npy\" ]]; then\n",
        "      echo \"✅ Skipping $DATASET_KEY ($TYPE) - output already exists.\"\n",
        "      continue\n",
        "    fi\n",
        "\n",
        "    echo \"🔄 Running feature extraction for $DATASET_KEY ($TYPE)\"\n",
        "    echo \"   Input dir: $DATA_DIR\"\n",
        "    echo \"   Output dir: $OUTPUT_DIR\"\n",
        "\n",
        "    mkdir -p \"$OUTPUT_DIR\"\n",
        "\n",
        "    python /content/drive/MyDrive/mad/code/ViT-SVM/src/eval_extract_embeddings_1inputdir.py \\\n",
        "      --n-gpu 1 \\\n",
        "      --model-arch l32 \\\n",
        "      --checkpoint-path /content/drive/MyDrive/mad/code/ViT-SVM/experiments/Pretrained_Imagenet_MTCNN/save/imagenet21k+imagenet2012_ViT-L_32.pth \\\n",
        "      --image-size 384 \\\n",
        "      --batch-size 32 \\\n",
        "      --num-workers 8 \\\n",
        "      --data-dir \"$DATA_DIR\" \\\n",
        "      --output-dir \"$OUTPUT_DIR\" \\\n",
        "      --num-classes 1000 \\\n",
        "      --part test\n",
        "  done\n",
        "done\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02jiP9Pmn_jG"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "# Split into train and test, unbalanced and balanced\n",
        "\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate myenv\n",
        "\n",
        "for DATASET_KEY in $(jq -r 'keys[]' dataset_configs.json); do\n",
        "  FEATURES_ROOT=$(jq -r --arg key \"$DATASET_KEY\" '.[$key].features_path' dataset_configs.json)\n",
        "  MORPH_DIR=\"${FEATURES_ROOT}/morph\"\n",
        "  BONAFIDE_DIR=\"${FEATURES_ROOT}/bonafide\"\n",
        "\n",
        "  echo \"🔄 Splitting train/test for dataset: $DATASET_KEY\"\n",
        "  echo \"   Morph features dir: $MORPH_DIR\"\n",
        "  echo \"   Bonafide features dir: $BONAFIDE_DIR\"\n",
        "\n",
        "  python /content/drive/MyDrive/mad/code/ViT-SVM/extracted_features/split_into_train_and_test.py \\\n",
        "    --morph_dir \"$MORPH_DIR\" \\\n",
        "    --bonafide_dir \"$BONAFIDE_DIR\"\n",
        "\n",
        "  python /content/drive/MyDrive/mad/code/ViT-SVM/extracted_features/split_into_train_and_test_balanced.py \\\n",
        "    --bonafide_dir \"$BONAFIDE_DIR\" \\\n",
        "    --morphs_dir \"$MORPH_DIR\"\n",
        "done\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sQatMiQoUph",
        "outputId": "055cd138-2dc6-4d03-882b-744a85fd9ec7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Checking if training is needed for: frll\n",
            "🔍 Checking if training is needed for: ffhq\n",
            "🔍 Checking if training is needed for: frll-cropped\n",
            "🔍 Checking if training is needed for: ffhq-cropped\n",
            "✅ Unbalanced SVM already trained for frll\n",
            "✅ Balanced SVM already trained for frll\n",
            "🏁 Finished: frll\n",
            "✅ Unbalanced SVM already trained for ffhq-cropped\n",
            "✅ Balanced SVM already trained for ffhq-cropped\n",
            "🏁 Finished: ffhq-cropped\n",
            "✅ Unbalanced SVM already trained for frll-cropped\n",
            "✅ Balanced SVM already trained for frll-cropped\n",
            "🏁 Finished: frll-cropped\n",
            "✅ Unbalanced SVM already trained for ffhq\n",
            "🚀 Training balanced SVM for ffhq...\n",
            "Test accuracy: 0.9722\n",
            "Confusion matrix:\n",
            "[[4873  130]\n",
            " [ 148 4855]]\n",
            "Balanced model saved to /content/drive/MyDrive/mad/code/ViT-SVM/trained_classifiers/ffhq-uncroppped/svm_balanced.pkl\n",
            "🏁 Finished: ffhq\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "# Parallel SVM training with skip logic based on model_path\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate myenv\n",
        "\n",
        "for DATASET_KEY in $(jq -r 'keys[]' dataset_configs.json); do\n",
        "  (\n",
        "    FEATURES_ROOT=$(jq -r --arg k \"$DATASET_KEY\" '.[$k].features_path' dataset_configs.json)\n",
        "    SAVE_DIR=$(jq -r --arg k \"$DATASET_KEY\" '.[$k].model_path' dataset_configs.json)\n",
        "\n",
        "    UNBALANCED_SAVE_DIR=\"${SAVE_DIR}/svm.pkl\"\n",
        "    BALANCED_SAVE_DIR=\"${SAVE_DIR}/svm_balanced.pkl\"\n",
        "\n",
        "    echo \"🔍 Checking if training is needed for: $DATASET_KEY\"\n",
        "\n",
        "    MORPH_FEATURES=\"$FEATURES_ROOT/morph\"\n",
        "    BONAFIDE_FEATURES=\"$FEATURES_ROOT/bonafide\"\n",
        "\n",
        "    if [ ! -f \"$UNBALANCED_SAVE_DIR\" ]; then\n",
        "      echo \"🚀 Training unbalanced SVM for $DATASET_KEY...\"\n",
        "      python /content/drive/MyDrive/mad/code/ViT-SVM/src/train_LinearSVM_MAD.py \\\n",
        "        --morph_path \"$MORPH_FEATURES\" \\\n",
        "        --bonafide_path \"$BONAFIDE_FEATURES\" \\\n",
        "        --save_dir \"$SAVE_DIR\"\n",
        "    else\n",
        "      echo \"✅ Unbalanced SVM already trained for $DATASET_KEY\"\n",
        "    fi\n",
        "\n",
        "    if [ ! -f \"$BALANCED_SAVE_DIR\" ]; then\n",
        "      echo \"🚀 Training balanced SVM for $DATASET_KEY...\"\n",
        "      python /content/drive/MyDrive/mad/code/ViT-SVM/src/train_LinearSVM_MAD_balanced.py \\\n",
        "        --morph_path \"$MORPH_FEATURES\" \\\n",
        "        --bonafide_path \"$BONAFIDE_FEATURES\" \\\n",
        "        --save_dir \"$SAVE_DIR\"\n",
        "    else\n",
        "      echo \"✅ Balanced SVM already trained for $DATASET_KEY\"\n",
        "    fi\n",
        "\n",
        "    echo \"🏁 Finished: $DATASET_KEY\"\n",
        "  ) &\n",
        "done\n",
        "\n",
        "wait  # Wait for all background jobs to finish\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VekTZPmEp9bU",
        "outputId": "05d18ad4-53cc-4fb1-d9b4-a8ae3db481da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Model: ffhq | Test: ffhq ===\n",
            "=== Model: ffhq | Test: ffhq-cropped ===\n",
            "=== Model: ffhq | Test: frll ===\n",
            "=== Model: ffhq | Test: frll-cropped ===\n",
            "🚀 Evaluation with model trained with unbalanced data ...\n",
            "🚀 Evaluation with model trained with unbalanced data ...\n",
            "🚀 Evaluation with model trained with unbalanced data ...\n",
            "🚀 Evaluation with model trained with unbalanced data ...\n",
            "=== Model: ffhq-cropped | Test: ffhq ===\n",
            "=== Model: ffhq-cropped | Test: ffhq-cropped ===\n",
            "=== Model: ffhq-cropped | Test: frll ===\n",
            "=== Model: ffhq-cropped | Test: frll-cropped ===\n",
            "🚀 Evaluation with model trained with unbalanced data ...\n",
            "🚀 Evaluation with model trained with unbalanced data ...\n",
            "🚀 Evaluation with model trained with unbalanced data ...\n",
            "🚀 Evaluation with model trained with unbalanced data ...\n",
            "=== Model: frll | Test: ffhq ===\n",
            "=== Model: frll | Test: ffhq-cropped ===\n",
            "=== Model: frll | Test: frll ===\n",
            "=== Model: frll | Test: frll-cropped ===\n",
            "🚀 Evaluation with model trained with unbalanced data ...\n",
            "🚀 Evaluation with model trained with unbalanced data ...\n",
            "🚀 Evaluation with model trained with unbalanced data ...\n",
            "🚀 Evaluation with model trained with unbalanced data ...\n",
            "=== Model: frll-cropped | Test: ffhq ===\n",
            "=== Model: frll-cropped | Test: ffhq-cropped ===\n",
            "=== Model: frll-cropped | Test: frll ===\n",
            "=== Model: frll-cropped | Test: frll-cropped ===\n",
            "🚀 Evaluation with model trained with unbalanced data ...\n",
            "🚀 Evaluation with model trained with unbalanced data ...\n",
            "🚀 Evaluation with model trained with unbalanced data ...\n",
            "🚀 Evaluation with model trained with unbalanced data ...\n",
            "Accuracy: 0.9725568942436412\n",
            "Accuracy: 0.9979367262723521\n",
            "Confusion Matrix:\n",
            " [[1410    3]\n",
            " [   0   41]]\n",
            "Confusion Matrix:\n",
            " [[1451    2]\n",
            " [  39    2]]\n",
            "Accuracy: 0.9484605087014726\n",
            "Accuracy: 0.9718019257221459\n",
            "Accuracy: 0.07965194109772424\n",
            "Accuracy: 0.8817056396148556\n",
            "Confusion Matrix:\n",
            " [[1415   38]\n",
            " [  39    2]]\n",
            "Confusion Matrix:\n",
            " [[1413    0]\n",
            " [  41    0]]\n",
            "Confusion Matrix:\n",
            " [[  78 1375]\n",
            " [   0   41]]\n",
            "Confusion Matrix:\n",
            " [[1270  143]\n",
            " [  29   12]]\n",
            "🚀 Evaluation with model trained with balanced data ...\n",
            "🚀 Evaluation with model trained with balanced data ...\n",
            "🚀 Evaluation with model trained with balanced data ...\n",
            "🚀 Evaluation with model trained with balanced data ...\n",
            "🚀 Evaluation with model trained with balanced data ...\n",
            "🚀 Evaluation with model trained with balanced data ...\n",
            "Accuracy: 0.9725568942436412\n",
            "Accuracy: 0.9698795180722891\n",
            "Accuracy: 0.9896836313617606\n",
            "Accuracy: 0.9718019257221459\n",
            "Confusion Matrix:\n",
            " [[1413   40]\n",
            " [   1   40]]\n",
            "Confusion Matrix:\n",
            " [[1398   15]\n",
            " [   0   41]]\n",
            "Confusion Matrix:\n",
            " [[1413    0]\n",
            " [  41    0]]\n",
            "Confusion Matrix:\n",
            " [[1446    7]\n",
            " [  38    3]]\n",
            "🏁 Done: frll → frll\n",
            "Accuracy: 0.969050894085282\n",
            "🏁 Done: frll-cropped → frll-cropped\n",
            "🏁 Done: frll-cropped → frll\n",
            "🏁 Done: frll → frll-cropped\n",
            "Accuracy: 0.2101740294511379\n",
            "Confusion Matrix:\n",
            " [[1406    7]\n",
            " [  38    3]]\n",
            "Confusion Matrix:\n",
            " [[ 273 1180]\n",
            " [   0   41]]\n",
            "🏁 Done: ffhq-cropped → frll-cropped\n",
            "🏁 Done: ffhq-cropped → frll\n",
            "Accuracy: 0.5909066989422723\n",
            "Confusion Matrix:\n",
            " [[4192  811]\n",
            " [6963 7037]]\n",
            "Accuracy: 0.8094910591471802\n",
            "Accuracy: 0.3498394990264695\n",
            "Accuracy: 0.8768406961178046\n",
            "Accuracy: 0.8049781613429459\n",
            "🚀 Evaluation with model trained with balanced data ...\n",
            "Accuracy: 0.5913803083723622\n",
            "Confusion Matrix:\n",
            " [[ 4440   563]\n",
            " [11792  2208]]\n",
            "Confusion Matrix:\n",
            " [[4201  802]\n",
            " [6963 7037]]\n",
            "Accuracy: 0.3587854549281692\n",
            "Accuracy: 0.7554070409935273\n",
            "Confusion Matrix:\n",
            " [[ 2650  2353]\n",
            " [ 2295 11705]]\n",
            "Confusion Matrix:\n",
            " [[1136  277]\n",
            " [   0   41]]\n",
            "🚀 Evaluation with model trained with balanced data ...\n",
            "Confusion Matrix:\n",
            " [[1298  155]\n",
            " [  29   12]]\n",
            "🏁 Done: frll-cropped → ffhq\n",
            "Confusion Matrix:\n",
            " [[ 4610   393]\n",
            " [11792  2208]]\n",
            "Confusion Matrix:\n",
            " [[ 1309  3694]\n",
            " [   12 13988]]\n",
            "🚀 Evaluation with model trained with balanced data ...\n",
            "Accuracy: 0.99884228805978\n",
            "Accuracy: 0.7582486975740672\n",
            "Confusion Matrix:\n",
            " [[ 2704  2299]\n",
            " [ 2295 11705]]\n",
            "Accuracy: 0.7294111456085881\n",
            "Confusion Matrix:\n",
            " [[ 1024  3979]\n",
            " [ 1163 12837]]\n",
            "🚀 Evaluation with model trained with balanced data ...\n",
            "🚀 Evaluation with model trained with balanced data ...\n",
            "🏁 Done: frll-cropped → ffhq-cropped\n",
            "🏁 Done: frll → ffhq\n",
            "🚀 Evaluation with model trained with balanced data ...\n",
            "Accuracy: 0.7504604536125875\n",
            "Confusion Matrix:\n",
            " [[ 1424  3579]\n",
            " [ 1163 12837]]\n",
            "🚀 Evaluation with model trained with balanced data ...\n",
            "🏁 Done: frll → ffhq-cropped\n",
            "Confusion Matrix:\n",
            " [[ 4993    10]\n",
            " [   12 13988]]\n",
            "Accuracy: 0.8159764247750355\n",
            "🚀 Evaluation with model trained with balanced data ...\n",
            "Accuracy: 0.9076305220883534\n",
            "Accuracy: 0.9270976616231087\n",
            "Confusion Matrix:\n",
            " [[1309  104]\n",
            " [   2   39]]\n",
            "Confusion Matrix:\n",
            " [[1342  111]\n",
            " [  27   14]]\n",
            "Confusion Matrix:\n",
            " [[ 1521  3482]\n",
            " [   15 13985]]\n",
            "🏁 Done: ffhq → frll-cropped\n",
            "🏁 Done: ffhq → frll\n",
            "Accuracy: 0.9992106509498501\n",
            "🏁 Done: ffhq-cropped → ffhq\n",
            "Confusion Matrix:\n",
            " [[ 5003     0]\n",
            " [   15 13985]]\n",
            "🏁 Done: ffhq-cropped → ffhq-cropped\n",
            "Accuracy: 0.9772667473556806\n",
            "Accuracy: 0.9801084039362206\n",
            "Confusion Matrix:\n",
            " [[ 4776   227]\n",
            " [  205 13795]]\n",
            "Confusion Matrix:\n",
            " [[ 4830   173]\n",
            " [  205 13795]]\n",
            "🚀 Evaluation with model trained with balanced data ...\n",
            "🚀 Evaluation with model trained with balanced data ...\n",
            "Accuracy: 0.9767931379255907\n",
            "Accuracy: 0.9756880492553808\n",
            "Confusion Matrix:\n",
            " [[ 4914    89]\n",
            " [  352 13648]]\n",
            "Confusion Matrix:\n",
            " [[ 4893   110]\n",
            " [  352 13648]]\n",
            "🏁 Done: ffhq → ffhq\n",
            "🏁 Done: ffhq → ffhq-cropped\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "# Parallel test of each SVM model against every test dataset\n",
        "\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate myenv\n",
        "\n",
        "# Get all dataset keys once\n",
        "DATASET_KEYS=$(jq -r 'keys[]' dataset_configs.json)\n",
        "\n",
        "for MODEL_KEY in $DATASET_KEYS; do\n",
        "  MODEL_DIR=$(jq -r --arg k \"$MODEL_KEY\" '.[$k].model_path' dataset_configs.json)\n",
        "  RESULTS_ROOT=$(jq -r --arg k \"$MODEL_KEY\" '.[$k].results_path' dataset_configs.json)\n",
        "\n",
        "  for TEST_KEY in $DATASET_KEYS; do\n",
        "    (\n",
        "      echo \"=== Model: $MODEL_KEY | Test: $TEST_KEY ===\"\n",
        "\n",
        "      FEATURES_ROOT=$(jq -r --arg k \"$TEST_KEY\" '.[$k].features_path' dataset_configs.json)\n",
        "      MORPH_FEATURES=\"$FEATURES_ROOT/morph\"\n",
        "      BONAFIDE_FEATURES=\"$FEATURES_ROOT/bonafide\"\n",
        "\n",
        "      CROSS_RESULTS_DIR=\"$RESULTS_ROOT/${TEST_KEY}\"\n",
        "      mkdir -p \"$CROSS_RESULTS_DIR\"\n",
        "\n",
        "      UNBALANCED_PATH=\"$CROSS_RESULTS_DIR/test_using_train_model_with_unbalanced_data.txt\"\n",
        "      BALANCED_PATH=\"$CROSS_RESULTS_DIR/test_using_train_model_with_balanced_data.txt\"\n",
        "\n",
        "      if [ ! -f \"$UNBALANCED_PATH\" ]; then\n",
        "        echo \"🚀 Evaluation with model trained with unbalanced data ...\"\n",
        "        python /content/drive/MyDrive/mad/code/ViT-SVM/src/eval_LinearSVM_MAD.py \\\n",
        "          --model_path \"$MODEL_DIR/svm.pkl\" \\\n",
        "          --morph_dir \"$MORPH_FEATURES\" \\\n",
        "          --bonafide_dir \"$BONAFIDE_FEATURES\" \\\n",
        "          --save_path \"$UNBALANCED_PATH\" \\\n",
        "          --test_files_suffix \"test_\"\n",
        "      else\n",
        "        echo \"✅ Unbalanced already exists: $UNBALANCED_PATH\"\n",
        "      fi\n",
        "\n",
        "      if [ ! -f \"$BALANCED_PATH\" ]; then\n",
        "        echo \"🚀 Evaluation with model trained with balanced data ...\"\n",
        "        python /content/drive/MyDrive/mad/code/ViT-SVM/src/eval_LinearSVM_MAD.py \\\n",
        "          --model_path \"$MODEL_DIR/svm_balanced.pkl\" \\\n",
        "          --morph_dir \"$MORPH_FEATURES\" \\\n",
        "          --bonafide_dir \"$BONAFIDE_FEATURES\" \\\n",
        "          --save_path \"$BALANCED_PATH\" \\\n",
        "          --test_files_suffix \"test_\"\n",
        "      else\n",
        "        echo \"✅ Balanced already exists: $BALANCED_PATH\"\n",
        "      fi\n",
        "\n",
        "      echo \"🏁 Done: $MODEL_KEY → $TEST_KEY\"\n",
        "    ) &\n",
        "  done\n",
        "done\n",
        "\n",
        "wait  # Wait for all parallel jobs to finish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_f37IU6jSOJx"
      },
      "outputs": [],
      "source": [
        "class DatasetConfig:\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_name: str,\n",
        "        is_landmark_occlusion: bool,\n",
        "        morph_images_path: str,\n",
        "        bonafide_images_path: str,\n",
        "        images_suffix: str,\n",
        "        features_path: str,\n",
        "        results_path: str\n",
        "    ):\n",
        "        self.dataset_name = dataset_name\n",
        "        self.is_landmark_occlusion = is_landmark_occlusion\n",
        "        self.morph_images_path = morph_images_path\n",
        "        self.bonafide_images_path = bonafide_images_path\n",
        "        self.images_suffix = images_suffix\n",
        "        self.features_path = features_path\n",
        "        self.results_path = results_path\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (\n",
        "            f\"DatasetConfig(dataset_name='{self.dataset_name}', \"\n",
        "            f\"is_landmark_occlusion={self.is_landmark_occlusion}, \"\n",
        "            f\"morph_images_path='{self.morph_images_path}', \"\n",
        "            f\"bonafide_images_path='{self.bonafide_images_path}', \"\n",
        "            f\"images_suffix='{self.images_suffix}', \"\n",
        "            f\"features_path='{self.features_path}', \"\n",
        "            f\"results_path='{self.results_path}')\"\n",
        "        )\n",
        "\n",
        "rows, cols = 4, 4\n",
        "\n",
        "occlusion_dataset_configs = {\n",
        "    \"ffhq-landmark-occlusion-mapping\": DatasetConfig(\n",
        "        dataset_name=\"FFHQ - Landmark Occlusion Mapping\",\n",
        "        is_landmark_occlusion=True,\n",
        "        morph_images_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/datasets/ffhq-cropped/morphs-occluded-regions/\",\n",
        "        bonafide_images_path=\"/content/drive/MyDrive/mad/datasets/ffhq-dataset/images1024x1024/\",\n",
        "        images_suffix=\"png\",\n",
        "        features_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/extracted_features/ffhq-landmark-occlusion/\",\n",
        "        results_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/ffhq-landmark-occlusion/\"\n",
        "    ),\n",
        "    \"ffhq-grid-occlusion-mapping\": DatasetConfig(\n",
        "        dataset_name=\"FFHQ - Grid Occlusion Mapping\",\n",
        "        is_landmark_occlusion=False,\n",
        "        morph_images_path=f\"/content/drive/MyDrive/mad/code/ViT-SVM/datasets/ffhq-cropped/morphs-grid-{rows}x{cols}-occluded/\",\n",
        "        bonafide_images_path=\"/content/drive/MyDrive/mad/datasets/ffhq-dataset/images1024x1024/\",\n",
        "        images_suffix=\"png\",\n",
        "        features_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/extracted_features/ffhq-grid-occlusion/\",\n",
        "        results_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/ffhq-grid-occlusion/\"\n",
        "    ),\n",
        "    \"frll-landmark-occlusion-mapping\": DatasetConfig(\n",
        "        dataset_name=\"FRLL - Landmark Occlusion Mapping\",\n",
        "        is_landmark_occlusion=True,\n",
        "        morph_images_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/datasets/frll-cropped/morphs-occluded-regions/\",\n",
        "        bonafide_images_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/datasets/frll-cropped/bonafide-occluded-regions/\",\n",
        "        images_suffix=\"jpg\",\n",
        "        features_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/extracted_features/frll-landmark-occlusion/\",\n",
        "        results_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/frll-landmark-occlusion/\"\n",
        "    ),\n",
        "    \"frll-grid-occlusion-mapping\": DatasetConfig(\n",
        "        dataset_name=\"FRLL - Grid Occlusion Mapping\",\n",
        "        is_landmark_occlusion=False,\n",
        "        morph_images_path=f\"/content/drive/MyDrive/mad/code/ViT-SVM/datasets/frll-cropped/morphs-grid-{rows}x{cols}-occluded/\",\n",
        "        bonafide_images_path=f\"/content/drive/MyDrive/mad/code/ViT-SVM/datasets/frll-cropped/bonafide-grid-{rows}x{cols}-occluded/\",\n",
        "        images_suffix=\"jpg\",\n",
        "        features_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/extracted_features/frll-grid-occlusion/\",\n",
        "        results_path=\"/content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/frll-grid-occlusion/\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "import json\n",
        "\n",
        "def serialize_dataset_configs(configs):\n",
        "    return {\n",
        "        k: {\n",
        "            \"dataset_name\": v.dataset_name,\n",
        "            \"is_landmark_occlusion\": v.is_landmark_occlusion,\n",
        "            \"morph_images_path\": v.morph_images_path,\n",
        "            \"bonafide_images_path\": v.bonafide_images_path,\n",
        "            \"images_suffix\": v.images_suffix,\n",
        "            \"features_path\": v.features_path,\n",
        "            \"results_path\": v.results_path,\n",
        "        }\n",
        "        for k, v in configs.items()\n",
        "    }\n",
        "\n",
        "# Save dataset configs\n",
        "with open(\"occlusion_dataset_configs.json\", \"w\") as f:\n",
        "    json.dump(serialize_dataset_configs(occlusion_dataset_configs), f)\n",
        "\n",
        "!cp occlusion_dataset_configs.json /content/drive/MyDrive/mad/code/ViT-SVM/configs/occlusion_dataset_configs.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0G9xKk4wQO9_",
        "outputId": "f773916c-8b3a-4f2f-b3b0-df999b380a67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Skipping ffhq-grid-occlusion-mapping (bonafide) - output already exists.\n",
            "✅ Skipping ffhq-grid-occlusion-mapping (morph) - output already exists.\n",
            "✅ Skipping ffhq-landmark-occlusion-mapping (bonafide) - output already exists.\n",
            "✅ Skipping ffhq-landmark-occlusion-mapping (morph) - output already exists.\n",
            "✅ Skipping frll-grid-occlusion-mapping (bonafide) - output already exists.\n",
            "✅ Skipping frll-grid-occlusion-mapping (morph) - output already exists.\n",
            "✅ Skipping frll-landmark-occlusion-mapping (bonafide) - output already exists.\n",
            "✅ Skipping frll-landmark-occlusion-mapping (morph) - output already exists.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "%%shell\n",
        "# Extract features for occlusion mapped datasets\n",
        "\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate myenv\n",
        "\n",
        "for DATASET_KEY in $(jq -r 'keys[]' occlusion_dataset_configs.json); do\n",
        "  for TYPE in bonafide morph; do\n",
        "    DATA_DIR=$(jq -r --arg key \"$DATASET_KEY\" --arg t \"$TYPE\" '.[$key][($t + \"_images_path\")]' occlusion_dataset_configs.json)\n",
        "    BASE_OUTPUT_DIR=$(jq -r --arg key \"$DATASET_KEY\" '.[$key].features_path' occlusion_dataset_configs.json)\n",
        "    OUTPUT_DIR=\"${BASE_OUTPUT_DIR}/${TYPE}\"   # Append morph or bonafide here\n",
        "\n",
        "    if [[ -f \"$OUTPUT_DIR/filenames.csv\" && -f \"$OUTPUT_DIR/features.npy\" ]]; then\n",
        "      echo \"✅ Skipping $DATASET_KEY ($TYPE) - output already exists.\"\n",
        "      continue\n",
        "    fi\n",
        "\n",
        "    echo \"🔄 Running feature extraction for $DATASET_KEY ($TYPE)\"\n",
        "    echo \"   Input dir: $DATA_DIR\"\n",
        "    echo \"   Output dir: $OUTPUT_DIR\"\n",
        "\n",
        "    mkdir -p \"$OUTPUT_DIR\"\n",
        "\n",
        "    python /content/drive/MyDrive/mad/code/ViT-SVM/src/eval_extract_embeddings_1inputdir.py \\\n",
        "      --n-gpu 1 \\\n",
        "      --model-arch l32 \\\n",
        "      --checkpoint-path /content/drive/MyDrive/mad/code/ViT-SVM/experiments/Pretrained_Imagenet_MTCNN/save/imagenet21k+imagenet2012_ViT-L_32.pth \\\n",
        "      --image-size 384 \\\n",
        "      --batch-size 32 \\\n",
        "      --num-workers 8 \\\n",
        "      --data-dir \"$DATA_DIR\" \\\n",
        "      --output-dir \"$OUTPUT_DIR\" \\\n",
        "      --num-classes 1000 \\\n",
        "      --part test\n",
        "  done\n",
        "done\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ymm33QFQdrLp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b93a42ab-6817-44b5-d96d-55ece6e8e905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== [START] Model: frll-cropped | Test: ffhq-grid-occlusion-mapping ===\n",
            "=== [START] Model: frll-cropped | Test: frll-landmark-occlusion-mapping ===\n",
            "=== [START] Model: frll-cropped | Test: frll-grid-occlusion-mapping ===\n",
            "=== [START] Model: ffhq-cropped | Test: frll-landmark-occlusion-mapping ===\n",
            "=== [START] Model: frll-cropped | Test: ffhq-landmark-occlusion-mapping ===\n",
            "=== [START] Model: ffhq-cropped | Test: frll-grid-occlusion-mapping ===\n",
            "=== [START] Model: ffhq-cropped | Test: ffhq-landmark-occlusion-mapping ===\n",
            "=== [START] Model: ffhq-cropped | Test: ffhq-grid-occlusion-mapping ===\n",
            "✅ Unbalanced exists: /content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/ffhq-landmark-occlusion//frll-cropped/test_using_train_model_with_unbalanced_data.txt\n",
            "✅ Balanced exists: /content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/ffhq-landmark-occlusion//frll-cropped/test_using_train_model_with_balanced_data.txt\n",
            "🏁 [DONE] Model: frll-cropped → Test: ffhq-landmark-occlusion-mapping\n",
            "✅ Unbalanced exists: /content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/ffhq-grid-occlusion//frll-cropped/test_using_train_model_with_unbalanced_data.txt\n",
            "✅ Balanced exists: /content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/ffhq-grid-occlusion//frll-cropped/test_using_train_model_with_balanced_data.txt\n",
            "🏁 [DONE] Model: frll-cropped → Test: ffhq-grid-occlusion-mapping\n",
            "✅ Unbalanced exists: /content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/frll-grid-occlusion//ffhq-cropped/test_using_train_model_with_unbalanced_data.txt\n",
            "✅ Balanced exists: /content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/frll-grid-occlusion//ffhq-cropped/test_using_train_model_with_balanced_data.txt\n",
            "🏁 [DONE] Model: ffhq-cropped → Test: frll-grid-occlusion-mapping\n",
            "✅ Unbalanced exists: /content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/frll-landmark-occlusion//ffhq-cropped/test_using_train_model_with_unbalanced_data.txt\n",
            "✅ Balanced exists: /content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/frll-landmark-occlusion//ffhq-cropped/test_using_train_model_with_balanced_data.txt\n",
            "🏁 [DONE] Model: ffhq-cropped → Test: frll-landmark-occlusion-mapping\n",
            "✅ Unbalanced exists: /content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/frll-landmark-occlusion//frll-cropped/test_using_train_model_with_unbalanced_data.txt\n",
            "✅ Balanced exists: /content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/frll-landmark-occlusion//frll-cropped/test_using_train_model_with_balanced_data.txt\n",
            "🏁 [DONE] Model: frll-cropped → Test: frll-landmark-occlusion-mapping\n",
            "✅ Unbalanced exists: /content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/frll-grid-occlusion//frll-cropped/test_using_train_model_with_unbalanced_data.txt\n",
            "✅ Balanced exists: /content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/frll-grid-occlusion//frll-cropped/test_using_train_model_with_balanced_data.txt\n",
            "🏁 [DONE] Model: frll-cropped → Test: frll-grid-occlusion-mapping\n",
            "🚀 Unbalanced Evaluation ...\n",
            "✅ Unbalanced exists: /content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/ffhq-landmark-occlusion//ffhq-cropped/test_using_train_model_with_unbalanced_data.txt\n",
            "✅ Balanced exists: /content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/ffhq-landmark-occlusion//ffhq-cropped/test_using_train_model_with_balanced_data.txt\n",
            "🏁 [DONE] Model: ffhq-cropped → Test: ffhq-landmark-occlusion-mapping\n",
            "Accuracy: 0.9428976734986421\n",
            "Confusion Matrix:\n",
            " [[373354  26837]\n",
            " [    12  69988]]\n",
            "✅ Balanced exists: /content/drive/MyDrive/mad/code/ViT-SVM/mad_scores/ffhq-grid-occlusion//ffhq-cropped/test_using_train_model_with_balanced_data.txt\n",
            "🏁 [DONE] Model: ffhq-cropped → Test: ffhq-grid-occlusion-mapping\n",
            "🎉 All evaluations completed!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "%%shell\n",
        "# Fully parallel evaluation: each SVM model (frll-cropped / ffhq-cropped) vs all datasets\n",
        "\n",
        "eval \"$(conda shell.bash hook)\"\n",
        "conda activate myenv\n",
        "\n",
        "# Define the two model paths\n",
        "MODEL_PATHS=(\n",
        "  \"/content/drive/MyDrive/mad/code/ViT-SVM/trained_classifiers/frll-cropped\"\n",
        "  \"/content/drive/MyDrive/mad/code/ViT-SVM/trained_classifiers/ffhq-cropped\"\n",
        ")\n",
        "\n",
        "DATASET_KEYS=$(jq -r 'keys[]' occlusion_dataset_configs.json)\n",
        "\n",
        "for MODEL_DIR in \"${MODEL_PATHS[@]}\"; do\n",
        "  MODEL_TYPE=$(basename \"$MODEL_DIR\")\n",
        "\n",
        "  for TEST_KEY in $DATASET_KEYS; do\n",
        "    # Run each combination in background\n",
        "    (\n",
        "      echo \"=== [START] Model: $MODEL_TYPE | Test: $TEST_KEY ===\"\n",
        "\n",
        "      FEATURES_ROOT=$(jq -r --arg k \"$TEST_KEY\" '.[$k].features_path' occlusion_dataset_configs.json)\n",
        "      RESULTS_ROOT=$(jq -r --arg k \"$TEST_KEY\" '.[$k].results_path' occlusion_dataset_configs.json)\n",
        "\n",
        "      MORPH_FEATURES=\"$FEATURES_ROOT/morph\"\n",
        "      BONAFIDE_FEATURES=\"$FEATURES_ROOT/bonafide\"\n",
        "      CROSS_RESULTS_DIR=\"${RESULTS_ROOT}/${MODEL_TYPE}\"\n",
        "\n",
        "      mkdir -p \"$CROSS_RESULTS_DIR\"\n",
        "\n",
        "      UNBALANCED_PATH=\"$CROSS_RESULTS_DIR/test_using_train_model_with_unbalanced_data.txt\"\n",
        "      BALANCED_PATH=\"$CROSS_RESULTS_DIR/test_using_train_model_with_balanced_data.txt\"\n",
        "\n",
        "      if [ ! -f \"$UNBALANCED_PATH\" ]; then\n",
        "        echo \"🚀 Unbalanced Evaluation ...\"\n",
        "        python /content/drive/MyDrive/mad/code/ViT-SVM/src/eval_LinearSVM_MAD.py \\\n",
        "          --model_path \"$MODEL_DIR/svm.pkl\" \\\n",
        "          --morph_dir \"$MORPH_FEATURES\" \\\n",
        "          --bonafide_dir \"$BONAFIDE_FEATURES\" \\\n",
        "          --save_path \"$UNBALANCED_PATH\" \\\n",
        "          --test_files_suffix \"\"\n",
        "      else\n",
        "        echo \"✅ Unbalanced exists: $UNBALANCED_PATH\"\n",
        "      fi\n",
        "\n",
        "      if [ ! -f \"$BALANCED_PATH\" ]; then\n",
        "        echo \"🚀 Balanced Evaluation ...\"\n",
        "        python /content/drive/MyDrive/mad/code/ViT-SVM/src/eval_LinearSVM_MAD.py \\\n",
        "          --model_path \"$MODEL_DIR/svm_balanced.pkl\" \\\n",
        "          --morph_dir \"$MORPH_FEATURES\" \\\n",
        "          --bonafide_dir \"$BONAFIDE_FEATURES\" \\\n",
        "          --save_path \"$BALANCED_PATH\" \\\n",
        "          --test_files_suffix \"\"\n",
        "      else\n",
        "        echo \"✅ Balanced exists: $BALANCED_PATH\"\n",
        "      fi\n",
        "\n",
        "      echo \"🏁 [DONE] Model: $MODEL_TYPE → Test: $TEST_KEY\"\n",
        "    ) &\n",
        "  done\n",
        "done\n",
        "\n",
        "wait  # Wait for all background jobs to finish\n",
        "echo \"🎉 All evaluations completed!\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from google.colab import runtime\n",
        "\n",
        "time.sleep(5 * 60)\n",
        "\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "f9S6TfYqU-Wp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}